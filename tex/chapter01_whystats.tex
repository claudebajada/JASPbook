% !TEX root = ../pdf/lsj.tex
% [There are multiple lsj.tex files, but the one in ../pdf is the usual one]



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Why do we learn statistics?~\label{ch:whystats}}


\begin{verse}{\it
``Thou shalt not answer questionnaires \\
Or quizzes upon World Affairs, \\
\hspace*{.5cm}    Nor with compliance \\
Take any test. Thou shalt not sit  \\
With statisticians nor commit \\
\hspace*{.5cm}    A social science" }\vspace*{6pt} \\ 
\hspace*{2cm} -- W.H. Auden\FOOTNOTE{The quote comes from Auden's 1946 poem {\it Under Which Lyre: A Reactionary Tract for the Times}, delivered as part of a commencement address at Harvard University. The history of the poem is kind of interesting: \url{http://harvardmagazine.com/2007/11/a-poets-warning.html}}
\end{verse}
\vspace*{12pt}


\section{The Role of Statistics in Medical Education~\label{sec:whywhywhy}}

For many medical students, it comes as a surprise that statistics plays a significant role in their training. It's safe to say that statistics is rarely the most favored subject in the medical curriculum. If you were genuinely passionate about statistics, chances are you'd be enrolled in a dedicated statistics course rather than pursuing a medical degree. Given this backdrop, it seems pertinent to start by addressing some common questions students have about the relevance of statistics in medicine.

One of the primary concerns is understanding what statistics is, its purpose, and why it's so integral to medical research. Scientists, particularly in the medical field, seem to rely heavily on statistical analysis. So much so that the rationale for it often goes unexplained. For many, the belief is almost axiomatic: your findings aren't credible until they've undergone statistical scrutiny. This might lead medical students to wonder:

\begin{quote}
{\it Why is there a need for statistics? Why not rely on \underline{clinical judgment} alone?}
\end{quote}

Though this might appear to be a simplistic question, it's a critical one to address. Many reasons can be cited, but perhaps the most straightforward one is that human judgment is fallible. We are all prone to biases, temptations, and errors that could significantly influence medical outcomes. Relying solely on "clinical judgment" to evaluate medical evidence would involve depending solely on intuition, anecdotal experiences, or the sheer reasoning power of the human mind. In the medical field, this is considered an unreliable approach for decision-making.

Exploring this issue further, we can question the trustworthiness of relying solely on "clinical judgment." While medical terminologies are framed in a specific language, language itself has its biases. Some concepts are more challenging to articulate, not necessarily because they are incorrect but because they are complex (e.g., the intricacies of cellular biology or the pharmacokinetics of a drug). Furthermore, our intuitive "gut feelings" are not designed to tackle complex medical issues; they are adapted for day-to-day problem-solving in a world that is rapidly evolving. At the core, making sound judgments requires the use of "induction," where one must extrapolate from available evidence to form general conclusions. If you believe you can make such extrapolations without being swayed by biases or inaccuracies, then that's a risky proposition in the medical setting, where lives are often at stake.

While statistics may not be everyone's favorite subject, it serves as an essential tool to counteract the limitations of human judgment and intuition. It brings an additional layer of rigor and objectivity, helping us make more informed and reliable decisions in medical practice.

\SUBSECTION{The Pitfall of Cognitive Bias in Medical Decision-Making}

Humans are remarkably intelligent beings, surpassing other species on Earth in cognitive abilities. This intelligence enables us to engage in complex reasoning and problem-solving. However, being highly intelligent doesn't exempt us from cognitive biases that can skew our judgement. One notable example of this is the \keyterm{belief bias effect} in logical reasoning. In medicine, just as in other fields, the ability to evaluate evidence impartially is crucial. The belief bias effect demonstrates that when assessing the validity of an argument—i.e., whether the conclusion logically follows from the premises—we tend to be influenced by how believable the conclusion appears to us.

Consider the following logically valid argument that aligns with common beliefs:
\begin{quote}
All cigarettes are expensive (Premise 1) \\
Some addictive things are inexpensive (Premise 2)\\
Therefore, some addictive things are not cigarettes (Conclusion)
\end{quote}
And here's a valid argument where the conclusion is not believable:
\begin{quote}
All addictive things are expensive (Premise 1)\\
Some cigarettes are inexpensive (Premise 2)\\
Therefore, some cigarettes are not addictive (Conclusion)
\end{quote}
The logical {\it structure} of argument \#2 is identical to the structure of argument \#1, and they're both valid. However, in the second argument, there are good reasons to think that premise 1 is incorrect, and as a result it's probably the case that the conclusion is also incorrect. But that's entirely irrelevant to the topic at hand; an argument is deductively valid if the conclusion is a logical consequence of the premises. That is, a valid argument doesn't have to involve true statements.

On the other hand, here's an invalid argument that has a believable conclusion:
\begin{quote}
All addictive things are expensive (Premise 1)\\
Some cigarettes are inexpensive (Premise 2)\\
Therefore, some addictive things are not cigarettes (Conclusion)
\end{quote}
And finally, an invalid argument with an unbelievable conclusion:
\begin{quote}
All cigarettes are expensive (Premise 1)\\
Some addictive things are inexpensive (Premise 2)\\
Therefore, some cigarettes are not addictive (Conclusion)
\end{quote}
Now, suppose that people really are perfectly able to set aside their pre-existing biases about what is true and what isn't, and purely evaluate an argument on its logical merits. We'd expect 100\% of people to say that the valid arguments are valid, and 0\% of people to say that the invalid arguments are valid. So if you ran an experiment looking at this, you'd expect to see data like this:

\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{l|cc|}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{conclusion feels true} & \multicolumn{1}{c}{conclusion feels false} \\ \cline{2-3}
argument is valid   & 100\% say ``valid'' & 100\%  say ``valid''\\ 
argument is invalid &  0\% say ``valid''& 0\% say ``valid''\\ \cline{2-3}
\end{tabular}
\end{center}

\noindent

In a study relevant to this topic  \parencite{Evans1983}, it was found that people are fairly good at identifying valid arguments when their beliefs align with the argument's conclusion. But when their beliefs run counter to a valid argument's conclusion, their ability to recognize its validity drops significantly. Furthermore, when people encounter an invalid argument that aligns with their pre-existing beliefs, they often fail to recognize its flaws.

What they found is that when pre-existing biases (i.e., beliefs) were in agreement with the structure of the data, everything went the way you'd hope: 
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{l|cc|}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{conclusion feels true} & \multicolumn{1}{c}{conclusion feels false} \\ \cline{2-3}
argument is valid   & 92\% say ``valid''&  \\ 
argument is invalid &  & 8\% say ``valid''\\ \cline{2-3}
\end{tabular}
\end{center}
Not perfect, but that's pretty good. But look what happens when our intuitive feelings about the truth of the conclusion run against the logical structure of the argument:
\begin{center}
\renewcommand{\arraystretch}{1.5}
\begin{tabular}{l|cc|}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{conclusion feels true} & \multicolumn{1}{c}{conclusion feels false} \\ \cline{2-3}
argument is valid   & 92\% say ``valid'' & {\bf 46\% say ``valid''} \\
argument is invalid & {\bf 92\% say ``valid''} & 8\% say ``valid'' \\ \cline{2-3}
\end{tabular}
\end{center}
Oh dear, that's not as good. Apparently, when people are presented with a strong argument that contradicts our pre-existing beliefs, we find it pretty hard to even perceive it to be a strong argument (people only did so 46\% of the time). Even worse, when people are presented with a weak argument that agrees with our pre-existing biases, almost no-one can see that the argument is weak (people got that one wrong 92\% of the time!).

While the data may not seem overly concerning, it's noteworthy that the accuracy in overcoming prior biases was about 60\%, which is better than the 50\% you'd expect by mere chance. Now, consider you're a medical professional tasked with diagnosing patients. If a tool could elevate your diagnostic accuracy from 60\% to, let's say, 95\%, wouldn't you want to utilize it? Absolutely, you would. Fortunately, such a tool exists: it's not magic, but statistics. This is one major reason why medical researchers value statistical methods. It's all too easy to fall prey to our own biases. If medical professionals could perfectly set aside cognitive biases, relying on intuition to evaluate data might be sufficient. However, that's far from the reality. The stakes in medical decision-making are extremely high, and even a moderate rate of error could have serious consequences. Statistics serves as a safeguard, helping us maintain objectivity and ensuring we rely on empirical evidence rather than subjective opinion. It helps keep us honest.

\section{The cautionary tale of Simpson's paradox}

%Clifford H. Wagner (February 1982). "Simpson's Paradox in Real Life". The American Statistician 36 (1): 46-48.

%Sex Bias in Graduate Admissions: Data from Berkeley
%P. J. Bickel, E. A. Hammel and J. W. O'Connell
%Science
%New Series, Vol. 187, No. 4175 (Feb. 7, 1975), pp. 398-404

The following is a true story (I think!). In 1973, the University of California, Berkeley had some worries about the admissions of students into their postgraduate courses. Specifically, the thing that caused the problem was that the gender breakdown of their admissions, which looked like this:
\begin{center}
\begin{tabular}{lcc}
& Number of applicants & Percent admitted \\
Males & 8442 & 44\% \\
Females & 4321 & 35\%  \\
\end{tabular}
\end{center}
Given this, they were worried about being sued!\footnote{Earlier versions of these notes incorrectly suggested that they actually were sued. But that's not true. There's a nice commentary on this here: \url{https://www.refsmmat.com/posts/2016-05-08-simpsons-paradox-berkeley.html}. A big thank you to Wilfried Van Hirtum for pointing this out to me.} Given that there were nearly 13,000 applicants, a difference of 9\% in admission rates between males and females is just way too big to be a coincidence. Pretty compelling data, right? And if I were to say to you that these data {\it actually} reflect a weak bias in favour of women (sort of!), you'd probably think that I was either crazy or sexist. 

Oddly, it's actually sort of true. When people started looking more carefully at the admissions data they told a rather different story \parencite{Bickel1975}. Specifically, when they looked at it on a department by department basis, it turned out that most of the departments actually had a slightly {\it higher} success rate for female applicants than for male applicants. The table below shows the admission figures for the six largest departments (with the names of the departments removed for privacy reasons):
\begin{center}
\begin{tabular}{c|cc|cc}
	& \multicolumn{2}{c|}{Males} & \multicolumn{2}{c}{Females} \\ 
	Department & Applicants	& Percent admitted &	Applicants & Percent admitted \\ \hline
A &	825	&62\%&	108&	82\% \\
B&	560	&63\%&	25	&68\% \\
C&	325	&37\%&	593	&34\%\\
D&	417	&33\%&	375	&35\%\\
E&	191	&28\%&	393	&24\%\\
F&	272	&6\%	&	341	&7\%\\
\end{tabular}
\end{center}
\noindent
Remarkably, most departments had a {\it higher} rate of admissions for females than for males! Yet the overall rate of admission across the university for females was {\it lower} than for males.  How can this be? How can both of these statements be true at the same time?

Here's what's going on. Firstly, notice that the departments are {\it not} equal to one another in terms of their admission percentages: some departments (e.g., A, B) tended to admit a high percentage of the qualified applicants, whereas others (e.g., F) tended to reject most of the candidates, even if they were high quality. So, among the six departments shown above, notice that department A is the most generous, followed by B, C, D, E and F in that order. Next, notice that males and females tended to apply to different departments. If we rank the departments in terms of the total number of male applicants, we get {\bf A}$>${\bf B}$>$D$>$C$>$F$>$E (the ``easy'' departments are in bold). On the whole, males tended to apply to the departments that had high admission rates. Now compare this to how the female applicants distributed themselves. Ranking the departments in terms of the total number of female applicants produces a quite different ordering C$>$E$>$D$>$F$>${\bf A}$>${\bf B}. In other words, what these data seem to be suggesting is that the female applicants tended to apply to ``harder'' departments. And in fact, if we look at Figure~\ref{fig:berkeley} we see that this trend is systematic, and quite striking. This effect is known as \keyterm{Simpson's paradox}. It's not common, but it does happen in real life, and most people are very surprised by it when they first encounter it, and many people refuse to even believe that it's real. It is very real. And while there are lots of very subtle statistical lessons buried in there, I want to use it to make a much more important point: doing research is hard, and there are {\it lots} of subtle, counter-intuitive traps lying in wait for the unwary.  That's reason \#2 why scientists love statistics, and why we teach research methods. Because science is hard, and the truth is sometimes cunningly hidden in the nooks and crannies of complicated data.


\begin{figure}[t!]
\begin{center}
\epsfig{file = ../img/whystats/berkeleyadmissions3.eps, clip=true,width = 11cm}
\caption[The Berkeley 1973 college admissions data.]{The Berkeley 1973 college admissions data. This figure plots the admission rate for the 85 departments that had at least one female applicant, as a function of the percentage of applicants that were female. The plot is a redrawing of Figure 1 from \textcite{Bickel1975}. Circles plot departments with more than 40 applicants; the area of the circle is proportional to the total number of applicants. The crosses plot departments with fewer than 40 applicants. } 
\label{fig:berkeley}
\HR
\end{center}
\end{figure}




Before leaving this topic entirely, I want to point out something else really critical that is often overlooked in a research methods class. Statistics only solves {\it part} of the problem. Remember that we started all this with the concern that Berkeley's admissions processes might be unfairly biased against female applicants. When we looked at the ``aggregated'' data, it did seem like the university was discriminating against women, but when we ``disaggregate'' and looked at the individual behaviour of all the departments, it turned out that the actual departments were, if anything, slightly biased in favour of women. The gender bias in total admissions was caused by the fact that women tended to self-select for harder departments.  From a legal perspective, that would probably put the university in the clear. Postgraduate admissions are determined at the level of the individual department, and there are good reasons to do that. At the level of individual departments the decisions are more or less unbiased (the weak bias in favour of females at that level is small, and not consistent across departments). Since the university can't dictate which departments people choose to apply to, and the decision making takes place at the level of the department it can hardly be held accountable for any biases that those choices produce. 

That was the basis for my somewhat glib remarks earlier, but that's not exactly the whole story, is it? After all, if we're interested in this from a more sociological and psychological perspective, we might want to ask {\it why} there are such strong gender differences in applications. Why do males tend to apply to engineering more often than females, and why is this reversed for the English department? And why is it the case that the departments that tend to have a female-application bias tend to have lower overall admission rates than those departments that have a male-application bias? Might this not still reflect a gender bias, even though every single department is itself unbiased? It might. Suppose, hypothetically, that males preferred to apply to ``hard sciences'' and females prefer ``humanities''. And suppose further that the reason for why the humanities departments have low admission rates is  because the government doesn't want to fund the humanities (Ph.D. places, for instance, are often tied to government funded research projects). Does that constitute a gender bias? Or just an unenlightened view of the value of the humanities? What if someone at a high level in the government cut the humanities funds because they felt that the humanities are ``useless chick stuff''. That seems pretty {\it blatantly} gender biased. None of this falls within the purview of statistics, but it matters to the research project. If you're interested in the overall structural effects of subtle gender biases, then you probably want to look at {\it both} the aggregated and disaggregated data. If you're interested in the decision making process at Berkeley itself then you're probably only interested in the disaggregated data. 

In short there are a lot of critical questions that you can't answer with statistics, but the answers to those questions will have a huge impact on how you analyse and interpret data. And this is the reason why you should always think of statistics as a {\it tool} to help you learn about your data. No more and no less. It's a powerful tool to that end, but there's no substitute for careful thought.


\section{Statistics in Medicine}

I trust that the preceding discussion has shed light on the general significance of statistics in the realm of science. Yet, you might still be wondering what specific role statistics plays in medicine, especially considering the emphasis placed on this subject in your medical curriculum. Here, I will attempt to clarify some of your queries.

\begin{itemize}

\item {\bf Why is statistics so prominent in medicine?}

Frankly, there are several reasons, each with its own level of importance. The most crucial aspect is that medicine is a statistical science. The "subjects" we examine are {\it patients}—complex, multidimensional, ever-changing individuals. Unlike the predictable elements of certain natural sciences, human physiology and pathology are far from static. Patients can respond unpredictably to treatments, have differing baseline conditions, and are influenced by a myriad of external factors such as lifestyle and genetics.

In essence, if you are going into the medical field, you will find that statistics is indispensable. Unlike some fields where the dictum might be ``if your experiment needs statistics, you should have designed a better experiment," medicine doesn't have this luxury. We are dealing with complex biological systems, not inanimate objects. Hence, understanding statistics is not a choice but a necessity.

\item {\bf Can't a specialist handle the statistics?}

While it's true you don't need to be a statistical expert to practice medicine, a basic level of proficiency in statistics is essential. Here are three reasons why:

\begin{itemize}
\item First, statistics and research design go hand-in-hand. Being good at one will inherently make you better at the other, especially when considering treatment efficacy and medical trials.
\item Second, most medical literature is replete with statistical data and analyses. Being able to understand this is vital for keeping up-to-date with medical advancements.
\item Third, employing a statistician for every piece of medical research is impractical and costly. Due to the shortage of trained statisticians, mastering basic statistical methods is not only practical but also economical.
\end{itemize}

\noindent
Moreover, these reasons are not limited to researchers alone. Even as a practicing clinician, you will benefit from being literate in statistics to interpret the latest findings in medical science.

\item {\bf What if I'm not interested in research or clinical practice? Do I still need statistics?}

This might sound like a rhetorical question, but the importance of statistics transcends vocational concerns. We live in a data-driven era, and statistical literacy is almost a life skill. Understanding statistical concepts will empower you to make informed decisions, whether in clinical practice, research, or even in understanding health trends in the media.

\end{itemize}


\section{Statistics in everyday life}

\begin{quote}
{\it ``We are drowning in information,\\ but we are starved for knowledge''} \\ \hspace*{2cm} -- Various authors, original probably John Naisbitt
\end{quote}

\noindent
When I started writing up my lecture notes I took the 20 most recent news articles posted to the ABC news website. Of those 20 articles, it turned out that 8 of them involved a discussion of something that I would call a statistical topic and 6 of those made a mistake. The most common error, if you're curious, was failing to report baseline data (e.g., the article mentions that 5\% of people in situation X have some characteristic Y, but doesn't say how common the characteristic is for everyone else!). The point I'm trying to make here isn't that journalists are bad at statistics (though they almost always are), it's that a  basic knowledge of statistics is very helpful for trying to figure out when someone else is either making a mistake or even lying to you. In fact, one of the biggest things that a knowledge of statistics does to you is cause you to get angry at the newspaper or the internet on a far more frequent basis. You can find a good example of this in Section~\ref{sec:housingpriceexample}. In later versions of this book I'll try to include more anecdotes along those lines. 


\section{There's more to research methods than statistics}

So far, most of what I've talked about is statistics, and so you'd be forgiven for thinking that statistics is all I care about in life. To be fair, you wouldn't be far wrong, but research methodology is a broader concept than statistics. So most research methods  courses will cover a lot of topics that relate much more to the pragmatics of research design, and in particular the issues that you encounter when trying to do research with humans. However, about 99\% of student {\it fears} relate to the statistics part of the course, so I've focused on the stats in this discussion, and hopefully I've convinced you that statistics matters, and more importantly, that it's not to be feared.  That being said, it's pretty typical for introductory research methods classes to be very stats-heavy. This is not (usually) because the lecturers are evil people. Quite the contrary, in fact. Introductory classes focus a lot on the statistics because you almost always find yourself needing statistics before you need the other research methods training. Why? Because almost all of your assignments in other classes will rely on statistical training, to a much greater extent than they rely on other methodological tools. It's not common for undergraduate  assignments to require you to design your own study from the ground up (in which case you would need to know a lot about research design), but it {\it is} common for assignments to ask you to analyse and interpret data that were collected in a study that someone else designed (in which case you need statistics). In that sense, from the perspective of allowing you to do well in all your other classes, the statistics is more urgent. 

But note that ``urgent'' is different from ``important'' -- they both matter. I really do want to stress that research design is just as important as data analysis, and this book does spend a fair amount of time on it. However, while statistics has a kind of universality, and provides a set of core tools that are useful for most types of medical research, the research methods side isn't quite so universal. There are some general principles that everyone should think about, but a lot of research design is very idiosyncratic, and is specific to the area of research that you want to engage in. To the extent that it's the details that matter, those details don't usually show up in an introductory stats and research methods class.


%\section{Plagiarism alert!}

%There's a lot of things I should explain about how I've organised this subject. Before I go into details, though, I want to acknowledge my sources. These notes came together from many years of reading, but there's some books in particular that I've relied upon. Some of them you might want to read yourself, others I strongly warn against. In order to spare you the tedium of seeing me cite the same few books over and over again, I'll just acknowledge them this once. The vast majority of the content in these notes is shamelessly lifted from the following books. On the off chance that any of these people ever read these notes, please take it as a compliment that I stole your stuff to teach to my students!

%\bigskip
%\begin{itemize}
%\item David Salsburg (2001). The lady tasting tea: How statistics revolutionized science in the twentieth century. \vspace*{6pt}

%{\it This is a lovely, easily readable discussion of the great debates in 20th century statistics. The W.H. Auden quote at the start of these notes is taken from the beginning of Salsburg's book. I thoroughly recommend reading the chapters on, Fisher and Neyman. As you'll see in this course, most of the bizarre pathologies in the statistical ``orthodoxies'' that we teach come from the fact that scientists have badly misunderstood the nature of their ideas.} \vspace*{6pt}

%\item Stephen Stigler (1986). The history of statistics: the measurement of uncertainty before 1900.  \vspace*{6pt}

%{\it This book is heavy going: unlike Salsburg's book, it's not intended as a mass-market book. It's a major piece of scholarship, and has all of the equations in the original form. It's a very rewarding read, since it tells you a lot about how the basic ideas about chance, probability and statistics were put together from the 17th century to the 19th century, but it's not for the light hearted!}  \vspace*{6pt}


%\item Andy Field (200X). Discovering statistics using SPSS (3rd ed).  \vspace*{6pt}

%{\it I have a love-hate relationship with this textbook. There's a lot of technical errors in the book (Field admits he's not a maths-y guy, and it kind of shows through to those of us who are!) and it's focused on a piece of software (SPSS) that we're trying to phase out, for reasons I'll explain later. For those reasons, I haven't set it as the textbook for this class. However, it's a bit of a pity, because there's a lot of things the book does extremely well. He writes in a really engaging way, and the book is bloody hilarious, insofar as that's possible for a stats textbook. Most importantly, he tries to explain that statistics aren't to be feared, and tries really hard to make the ideas accessible. A lot of my thinking about how to approach a subject that most people hate has been shaped by his book.}  \vspace*{6pt}


%\item William Hays (199x). Statistics (5th ed). \vspace*{6pt}

%{\it This is a textbook aimed at postgraduate students in psychology. It's actually the one that my wife learned most of her stats from, and she's a statistician now. It's a bit advanced for this course, but I used it a lot in thinking about what kinds of skills I should be setting you up for.}  \vspace*{6pt}

%\item Larry Wasserman (200x). All of statistics.  \vspace*{6pt}

%\item Robert Hogg, Joseph McKean \& Allen Craig (2005). Introduction to mathematical statistics (6th ed)  \vspace*{6pt}

%\item Mark Schervish (1995). The theory of statistics (2nd ed).  \vspace*{6pt}

%{\it Wasserman, Hogg et al \& Schervish are all very advanced texts. Psychology undergrads should never read these. Seriously. It took me about 10 years to be able to get to the point where I could read Schervish, which is the most hardcore of the three. I used these textbooks to make sure what I'm teaching you is actually right, and to make sure I understood all the maths underneath the lectures, but it's way, way beyond the scope of this class to look at that sort of thing!}  \vspace*{6pt}

%\end{itemize}

